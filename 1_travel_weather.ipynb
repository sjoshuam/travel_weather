{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54f664f8-58d2-4415-82d6-f2ad15f77268",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########----------##########----------##########----------##########----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf05299-44af-42e4-9e1b-cf5a7be27e91",
   "metadata": {},
   "source": [
    "# Libraries, setup, and general use objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6174b868-46b2-4ab1-a0d1-18881bf2dc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as url\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipyparallel as ipp\n",
    "import pickle\n",
    "import datetime as dt\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from os import listdir, mkdir\n",
    "from os.path import isdir, isfile\n",
    "\n",
    "from sklearn.metrics   import f1_score\n",
    "from sklearn.metrics   import mean_absolute_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble  import RandomForestClassifier\n",
    "from sklearn.svm       import SVC\n",
    "\n",
    "set_gather_data = False\n",
    "set_sample_frac = 1 / 20\n",
    "set_parallel_cores = {'Download': 3, 'Model':12}\n",
    "\n",
    "def time_check(s = 'A'):\n",
    "    print('Time Check: Point ' + s)\n",
    "    print(dt.datetime.now())\n",
    "\n",
    "##\n",
    "valid_prefix = [69, 72, 74, 99]\n",
    "use_col_list = {'DATE':str, 'LATITUDE':float, 'LONGITUDE':float,\n",
    "    'ELEVATION':float, \"HourlyDryBulbTemperature\":float,\n",
    "                \"HourlyPrecipitation\":float} # \"HourlyRelativeHumidity\":float,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffcc824-50a7-4b00-9ca2-dca03ba27d03",
   "metadata": {},
   "source": [
    "#### Generate directories as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1282f5a0-7fb2-4813-9b4c-118e866916fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directories():\n",
    "    all_dirs = ['A_Input', 'B_Process', 'C_Output']\n",
    "    all_dirs = all_dirs + ['B_Process/downloads', 'B_Process/model_data']\n",
    "    for i in all_dirs:\n",
    "        if not isdir(i): mkdir(i)\n",
    "        \n",
    "make_directories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91d709e-a93a-4467-944a-6fc491586579",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e293069b-6f3c-4db1-83e2-e2a2680497b7",
   "metadata": {},
   "source": [
    "#### read_year_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe681d5f-4ab4-43a3-8e88-8cfc99f45508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_year_links(dir_address = 'A_Input/year_links.txt'):\n",
    "    the_connection = open(dir_address, 'r')\n",
    "    year_links = the_connection.readlines()\n",
    "    the_connection.close()\n",
    "    year_links = [i.strip() for i in year_links]\n",
    "    return year_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e4c2ff-ed58-4515-be38-d44b4003f654",
   "metadata": {},
   "source": [
    "#### extract_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b86605-4ec2-4a33-ab43-5029b5c911d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(address):\n",
    "    \n",
    "    ## retrieve raw web page\n",
    "    url_connect = url.urlopen(address)\n",
    "    all_links = url_connect.read()\n",
    "    url_connect.close()\n",
    "    \n",
    "    ## extract all links to csv files\n",
    "    all_links = BeautifulSoup(all_links)\n",
    "    all_links = all_links.find_all('a')\n",
    "    all_links = [i.string for i in all_links if i.string.find('.csv') != -1]\n",
    "    \n",
    "    ## eliminate files unlikely to represent US weather stations\n",
    "    def us_range(x, target_range = valid_prefix):\n",
    "        try:\n",
    "            int(x[0:2])\n",
    "        except:\n",
    "            return False\n",
    "        if int(x[0:2]) in target_range:\n",
    "            return True\n",
    "        else:\n",
    "             return False\n",
    "    all_links = filter(us_range, all_links)\n",
    "    \n",
    "    ## remove url if files have already been downloaded\n",
    "    already_downloaded = listdir('B_Process/downloads')\n",
    "    valid_links = list()\n",
    "    for i in all_links:\n",
    "        if address[-5:-1] + '_' + i + '.gz' in already_downloaded:\n",
    "            pass\n",
    "        else:\n",
    "            x = address[-5:-1] + '_' + i + '.gz'\n",
    "            y = address + i\n",
    "            valid_links.append((x, y))\n",
    "            \n",
    "    return valid_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb37744d-eb60-44e8-a500-f15b36a88504",
   "metadata": {},
   "source": [
    "#### download_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "163e0c2b-1c70-4e44-a6d0-1215441eea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(links_ext, ucl = use_col_list):\n",
    "    import pandas as pd\n",
    "    try:\n",
    "        the_csv = pd.read_csv(links_ext[1], usecols = list(ucl.keys()),\n",
    "            parse_dates = ['DATE'], dtype = ucl)\n",
    "    except:\n",
    "        the_csv = pd.read_csv(links_ext[1], usecols = list(ucl.keys()),\n",
    "            parse_dates = ['DATE'], dtype = str)\n",
    "        for j in ucl.keys():\n",
    "            if ucl[j] == float:\n",
    "                the_csv[j] = pd.to_numeric(the_csv[j], errors = 'coerce')\n",
    "        \n",
    "    the_csv = the_csv.round(3)\n",
    "    the_csv.to_csv('B_Process/downloads/' + links_ext[0], encoding = 'utf-8',\n",
    "                    index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a36cb1a-4b4b-44d7-b343-0073e50c9a54",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Execute code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e20343e-6dcb-450b-921e-f08e85bfcc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_gather_data:\n",
    "\n",
    "    ## read in list of links to file directory pages for each year of the data\n",
    "    year_links = read_year_links()\n",
    "\n",
    "    ## iterative extract files from the links on each directory page\n",
    "    for i in year_links:\n",
    "        links_extracted = extract_links(i)\n",
    "        if len(links_extracted) < 1: continue\n",
    "    \n",
    "        with ipp.Cluster(n = set_parallel_cores['Download']) as rc:\n",
    "            par_processes = rc.load_balanced_view()\n",
    "            par_result = par_processes.map_async(download_files, links_extracted)\n",
    "            par_result.wait_interactive()\n",
    "            final_result = par_result.get()\n",
    "            del par_processes, par_result, final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f529ee97-e732-4edc-9c03-18abab2615b1",
   "metadata": {},
   "source": [
    "# Refine Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34be5d38-1b2a-4539-9f56-017b592c3866",
   "metadata": {},
   "source": [
    "#### refine_data (and compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49da520a-dcec-4c76-b23d-4aede2035515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_data(file_dir, segment, ucl = use_col_list,\n",
    "                sample_fraction = set_sample_frac):\n",
    "    \n",
    "    ## generate roster of files\n",
    "    list_files = listdir(file_dir)\n",
    "    list_files = [i for i in list_files if i[0] != '.']\n",
    "    \n",
    "    ## filter files to specified segment\n",
    "    def us_range(x, target_range = segment):\n",
    "        try:\n",
    "            x[0:7]\n",
    "        except:\n",
    "            return False\n",
    "        if x[0:7] in target_range:\n",
    "            return True\n",
    "        else:\n",
    "             return False\n",
    "    list_files = filter(us_range, list_files)\n",
    "    \n",
    "    ## assemble files\n",
    "    all_data = list()\n",
    "    for i in list_files:\n",
    "        file_iter = pd.read_csv(file_dir + '/' + i, parse_dates = ['DATE'],\n",
    "                                dtype = ucl)\n",
    "        \n",
    "        ## deconstruct day/times\n",
    "        file_iter['Day'] = file_iter['DATE'].dt.dayofyear\n",
    "        file_iter['Hour'] = file_iter['DATE'].dt.hour\n",
    "        file_iter = file_iter.drop('DATE', axis = 1)\n",
    "        \n",
    "        ## score weather\n",
    "        file_iter['HourlyPrecipitation'] = file_iter[\n",
    "            'HourlyPrecipitation'].fillna(0)\n",
    "        file_iter['Temperate'] = (file_iter['HourlyDryBulbTemperature'] > 55) &\\\n",
    "            (file_iter['HourlyDryBulbTemperature'] < 75) &\\\n",
    "            (file_iter['HourlyPrecipitation'] < 0.1)\n",
    "        file_iter['Temperate'] = file_iter['Temperate'].astype(int)\n",
    "        file_iter = file_iter.drop(['HourlyDryBulbTemperature',\n",
    "            'HourlyPrecipitation'], axis = 1)\n",
    "        \n",
    "        ## rename columns to make capitalization consistent\n",
    "        file_iter = file_iter.rename(columns = {'LATITUDE':'Lat',\n",
    "            'LONGITUDE': 'Lon', 'ELEVATION':\"Elev\"})\n",
    "        \n",
    "        ## sample data if specified\n",
    "        if sample_fraction < 1:\n",
    "            assert sample_fraction > 0\n",
    "            sample_n = file_iter.shape[0] * sample_fraction\n",
    "            sample_n = int(sample_n)\n",
    "            sample_n = np.max([sample_n, int((0.5**2)/(0.05**2))])\n",
    "            sample_n = np.min([sample_n, file_iter.shape[0]])\n",
    "            file_iter = file_iter.sample(\n",
    "                n = int(sample_n),\n",
    "                weights = (file_iter['Temperate'] * 1) + 1\n",
    "            )\n",
    "\n",
    "        ## drop files outside the US's rough lat/lon box; append others\n",
    "        if max(file_iter.Lat) > 25 and min(file_iter.Lat) < 50:\n",
    "            if max(file_iter.Lon) < -60 and min(file_iter.Lon) > -130:\n",
    "                all_data.append(file_iter)\n",
    "\n",
    "    ## compile files and save\n",
    "    if len(all_data) > 0:\n",
    "        all_data = pd.concat(all_data, axis = 0)\n",
    "        all_data.to_csv('B_Process/model_data/' + str(segment[0]) +\\\n",
    "            '_weather_data.csv.gz', index = False, encoding = 'utf-8')\n",
    "        return all_data\n",
    "    else:\n",
    "        pass\n",
    "        #print('WARNING: ' + str(segment[0]) + ' files contain no valid data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ca0fae-d284-4668-b492-c09354b59035",
   "metadata": {},
   "source": [
    "#### load_model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "457638df-f721-4ce1-b4aa-9b8ce66168cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_data(file_directory = 'B_Process/model_data'):\n",
    "    all_data = listdir(file_directory)\n",
    "    all_data = [i for i in all_data if i[-6:] == 'csv.gz']\n",
    "    for i in range(len(all_data)):\n",
    "        all_data[i] = pd.read_csv(file_directory + '/' + all_data[i])\n",
    "    all_data = pd.concat(all_data, axis = 0)\n",
    "    all_data.reset_index()\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3348c2-11a4-4644-89db-64ddbf6af7db",
   "metadata": {},
   "source": [
    "#### Execute Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "039fa2fb-b1e0-4eae-9cdf-68505a3a5160",
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_gather_data:\n",
    "    for i in valid_prefix:\n",
    "        for j in range(2015, 2020):\n",
    "            segment_iter = [str(j) + '_' + str(i)]\n",
    "            refine_data('B_Process/downloads', segment = segment_iter)\n",
    "\n",
    "model_data = load_model_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad05bf5-0483-4eab-9eec-d5b4031727d2",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df15523d-3a01-4e39-ad65-eabc1732f483",
   "metadata": {},
   "source": [
    "#### split data into train and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a48b9bef-d62d-4efd-9319-8223b911ef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split data into train and test subsets\n",
    "model_data.loc[:, 'Split'] = np.random.binomial(\n",
    "    n = 1, size = (model_data.shape[0],), p = 0.8).astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802df092-2db8-4ff7-9815-7c805d7dcda7",
   "metadata": {},
   "source": [
    "#### model_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d87f2869-fd9a-4ea5-9024-45e0cc22fb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather Model F1: 0.833 (Cutoff = 0.5)\n"
     ]
    }
   ],
   "source": [
    "def model_weather(dat = model_data):\n",
    "    \n",
    "    ## split data into train and test data\n",
    "    test_data = dat[~dat['Split']]\n",
    "    dat = dat[dat['Split']]\n",
    "    \n",
    "    ## round off data and average for each rounded grid\n",
    "    simple_dat = dat.drop(['Elev', 'Split'], axis = 1).round()\n",
    "    simple_dat.loc[:, 'Day']  = np.ceil(simple_dat.loc[:, 'Day']  / 5) * 5\n",
    "    simple_dat.loc[:, 'Hour'] = np.ceil(simple_dat.loc[:, 'Hour'] / 2) * 2\n",
    "    simple_dat.loc[:, ['Lon', 'Lat', 'Day', 'Hour']] = simple_dat.astype(int)\n",
    "    simple_dat = simple_dat.groupby(['Lon', 'Lat', 'Day', 'Hour']).mean()\n",
    "    simple_dat = simple_dat.reset_index()\n",
    "    \n",
    "    ## train k nearest neighbor model\n",
    "    knn_model = KNeighborsRegressor(weights = 'distance')\n",
    "    knn_model = knn_model.fit(\n",
    "                                simple_dat[['Lon', 'Lat', 'Day', 'Hour']],\n",
    "                                simple_dat['Temperate']\n",
    "                                )\n",
    "    \n",
    "    ## construct predictor function (using Python context-saving feature)\n",
    "    def find_closest_mean(new_data):\n",
    "        new_data = new_data[['Lon', 'Lat', 'Day', 'Hour']]\n",
    "        return knn_model.predict(new_data)\n",
    "    \n",
    "    ## announce accuracy of predictor function\n",
    "    ml_score = find_closest_mean(test_data) > 0.5\n",
    "    ml_score = ml_score.astype(int)\n",
    "    ml_score = f1_score(test_data['Temperate'].values, ml_score).round(3)\n",
    "    print('Weather Model F1: ' + str(ml_score) + ' (Threshold = 0.5)')\n",
    "    \n",
    "    return find_closest_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff88c9b4-734e-4951-9478-91ac10c9a08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_temperate(dat, chron_mod, elev_mod):\n",
    "    \n",
    "    ## prepare data\n",
    "    dat.loc[:,'Elev'] = np.nan\n",
    "    dat.loc[:, 'Chron'] = np.nan\n",
    "    test_data = dat[~dat['Split']].drop('Split', axis = 1)\n",
    "    dat = dat[dat['Split']].drop('Split', axis = 1)\n",
    "    \n",
    "    ## model elevation and chronology for training\n",
    "    dat['Elev'] = elev_mod(dat)\n",
    "    dat['Chron'] = chron_mod(dat)\n",
    "    \n",
    "#    ## declare model - random forest\n",
    "#    main_model = RandomForestClassifier(\n",
    "#        n_jobs = set_parallel_cores['Model'],\n",
    "#        max_features = None,\n",
    "#        n_estimators = np.math.factorial(4)\n",
    "#        )\n",
    "\n",
    "    ## declare model - support vector machine (+ scale data)\n",
    "    dat['Lon'] = (dat['Lon'] + 130)/ (-60 + 130)\n",
    "    dat['Lat'] = (dat['Lat'] - 25) / (50 - 25)\n",
    "    dat['Elev'] = dat['Elev'] / 5280\n",
    "    main_model = SVC(kernel = 'rbf')\n",
    "    \n",
    "    ## fit model\n",
    "    main_model = main_model.fit(\n",
    "        dat[['Lon', 'Lat', 'Elev', 'Chron']],\n",
    "        dat['Temperate']\n",
    "        )\n",
    "    \n",
    "    ## construct predictor function\n",
    "    def temperate_predictor(x_pd):\n",
    "        x_pd['Elev'] = elev_mod(x_pd)\n",
    "        x_pd['Chron'] = chron_mod(x_pd)\n",
    "        x_pd = x_pd[['Lon', 'Lat', 'Elev', 'Chron']]\n",
    "        return main_model.predict(x_pd)\n",
    "\n",
    "    ## annouce accuracy of predictor function\n",
    "    ml_score = temperate_predictor(test_data)\n",
    "    ml_score = ml_score.astype(int)\n",
    "    ml_score = f1_score(test_data['Temperate'].values, ml_score).round(3)\n",
    "    print('Temperate Weather Model - F1 Score is: ' + str(ml_score))\n",
    "    \n",
    "    return temperate_predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439e9ea6-f04d-43dd-8cf9-517192460ad8",
   "metadata": {},
   "source": [
    "#### Execute Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e60a142-a4fd-4e74-b842-b524faa2ba28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Check: Point A\n",
      "2022-03-04 09:17:39.659477\n",
      "Weather Model F1: 0.833 (Cutoff = 0.5)\n",
      "Time Check: Point Z\n",
      "2022-03-04 09:17:49.361891\n"
     ]
    }
   ],
   "source": [
    "time_check('A')\n",
    "weather_model = model_weather()\n",
    "time_check('Z')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ddf3c4-792a-4f0d-90a7-94d631ffb412",
   "metadata": {},
   "source": [
    "# Model Routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7495db0f-0a5d-4124-ab73-dd0ac693c0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a337981f-da13-4141-a926-1d95230b30a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24382e0-9187-4470-8e3b-6969112a5d6e",
   "metadata": {},
   "source": [
    "# Render Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0e1e62-9856-4150-a367-8201a073e107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
