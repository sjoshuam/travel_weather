{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54f664f8-58d2-4415-82d6-f2ad15f77268",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########----------##########----------##########----------##########----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf05299-44af-42e4-9e1b-cf5a7be27e91",
   "metadata": {},
   "source": [
    "# Libraries, setup, and general use objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6174b868-46b2-4ab1-a0d1-18881bf2dc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Check: Point A\n",
      "2022-03-04 10:56:56.276555\n",
      "\n",
      "WARNING: Copies of city_list.xlsx used in two different projects.\n",
      "         Treat the us_travels version as the master version.\n",
      "         Sync this travel_weather version to it.\n"
     ]
    }
   ],
   "source": [
    "## import full libraries\n",
    "import urllib.request as url\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipyparallel as ipp\n",
    "import pickle\n",
    "import datetime as dt\n",
    "\n",
    "## import needed functons from libraries\n",
    "from bs4 import BeautifulSoup\n",
    "from os import listdir, mkdir\n",
    "from os.path import isdir, isfile\n",
    "from sklearn.metrics   import f1_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "## define settings\n",
    "set_gather_data = False\n",
    "set_sample_frac = 1 / 5\n",
    "set_parallel_cores = {'Download': 3, 'Model':12}\n",
    "valid_prefix = [69, 72, 74, 99]\n",
    "use_col_list = {'DATE':str, 'LATITUDE':float, 'LONGITUDE':float,\n",
    "    'ELEVATION':float, \"HourlyDryBulbTemperature\":float,\n",
    "                \"HourlyPrecipitation\":float} # \"HourlyRelativeHumidity\":float,\n",
    "set_geo_box = {'Lon':(-125, -60), 'Lat':(25, 50)}\n",
    "\n",
    "## define simple timing function\n",
    "def time_check(s = 'A'):\n",
    "    print('Time Check: Point ' + s)\n",
    "    print(dt.datetime.now())\n",
    "time_check('A')\n",
    "\n",
    "## read in general purpose input files\n",
    "with open('A_Input/year_links.txt', 'r') as conn:\n",
    "    year_links = conn.readlines()\n",
    "    conn.close()\n",
    "    \n",
    "city_list = pd.read_excel('A_Input/city_list.xlsx')\n",
    "city_list = city_list.rename({'lon':'Lon', 'lat':'Lat'}, axis = 1)\n",
    "\n",
    "print(\n",
    "    '\\n' +\\\n",
    "    'WARNING: Copies of city_list.xlsx used in two different projects.\\n' +\\\n",
    "    '         Treat the us_travels version as the master version.\\n' +\\\n",
    "    '         Sync this travel_weather version to it.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffcc824-50a7-4b00-9ca2-dca03ba27d03",
   "metadata": {},
   "source": [
    "#### Generate directories as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1282f5a0-7fb2-4813-9b4c-118e866916fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directories():\n",
    "    all_dirs = ['A_Input', 'B_Process', 'C_Output']\n",
    "    all_dirs = all_dirs + ['B_Process/downloads', 'B_Process/model_data']\n",
    "    for i in all_dirs:\n",
    "        if not isdir(i): mkdir(i)\n",
    "        \n",
    "make_directories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91d709e-a93a-4467-944a-6fc491586579",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e4c2ff-ed58-4515-be38-d44b4003f654",
   "metadata": {},
   "source": [
    "#### extract_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b86605-4ec2-4a33-ab43-5029b5c911d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(address):\n",
    "    \n",
    "    ## retrieve raw web page\n",
    "    url_connect = url.urlopen(address)\n",
    "    all_links = url_connect.read()\n",
    "    url_connect.close()\n",
    "    \n",
    "    ## extract all links to csv files\n",
    "    all_links = BeautifulSoup(all_links)\n",
    "    all_links = all_links.find_all('a')\n",
    "    all_links = [i.string for i in all_links if i.string.find('.csv') != -1]\n",
    "    \n",
    "    ## eliminate files unlikely to represent US weather stations\n",
    "    def us_range(x, target_range = valid_prefix):\n",
    "        try:\n",
    "            int(x[0:2])\n",
    "        except:\n",
    "            return False\n",
    "        if int(x[0:2]) in target_range:\n",
    "            return True\n",
    "        else:\n",
    "             return False\n",
    "    all_links = filter(us_range, all_links)\n",
    "    \n",
    "    ## remove url if files have already been downloaded\n",
    "    already_downloaded = listdir('B_Process/downloads')\n",
    "    valid_links = list()\n",
    "    for i in all_links:\n",
    "        if address[-5:-1] + '_' + i + '.gz' in already_downloaded:\n",
    "            pass\n",
    "        else:\n",
    "            x = address[-5:-1] + '_' + i + '.gz'\n",
    "            y = address + i\n",
    "            valid_links.append((x, y))\n",
    "            \n",
    "    return valid_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb37744d-eb60-44e8-a500-f15b36a88504",
   "metadata": {},
   "source": [
    "#### download_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "163e0c2b-1c70-4e44-a6d0-1215441eea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(links_ext, ucl = use_col_list):\n",
    "    import pandas as pd\n",
    "    try:\n",
    "        the_csv = pd.read_csv(links_ext[1], usecols = list(ucl.keys()),\n",
    "            parse_dates = ['DATE'], dtype = ucl)\n",
    "    except:\n",
    "        the_csv = pd.read_csv(links_ext[1], usecols = list(ucl.keys()),\n",
    "            parse_dates = ['DATE'], dtype = str)\n",
    "        for j in ucl.keys():\n",
    "            if ucl[j] == float:\n",
    "                the_csv[j] = pd.to_numeric(the_csv[j], errors = 'coerce')\n",
    "        \n",
    "    the_csv = the_csv.round(3)\n",
    "    the_csv.to_csv('B_Process/downloads/' + links_ext[0], encoding = 'utf-8',\n",
    "                    index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a36cb1a-4b4b-44d7-b343-0073e50c9a54",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Execute code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e20343e-6dcb-450b-921e-f08e85bfcc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_gather_data:\n",
    "\n",
    "    ## read in list of links to file directory pages for each year of the data\n",
    "    year_links = read_year_links()\n",
    "\n",
    "    ## iterative extract files from the links on each directory page\n",
    "    for i in year_links:\n",
    "        links_extracted = extract_links(i)\n",
    "        if len(links_extracted) < 1: continue\n",
    "    \n",
    "        with ipp.Cluster(n = set_parallel_cores['Download']) as rc:\n",
    "            par_processes = rc.load_balanced_view()\n",
    "            par_result = par_processes.map_async(download_files, links_extracted)\n",
    "            par_result.wait_interactive()\n",
    "            final_result = par_result.get()\n",
    "            del par_processes, par_result, final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f529ee97-e732-4edc-9c03-18abab2615b1",
   "metadata": {},
   "source": [
    "# Refine Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34be5d38-1b2a-4539-9f56-017b592c3866",
   "metadata": {},
   "source": [
    "#### refine_data (and compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49da520a-dcec-4c76-b23d-4aede2035515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_data(file_dir, segment, ucl = use_col_list,\n",
    "                sample_fraction = set_sample_frac):\n",
    "    \n",
    "    ## generate roster of files\n",
    "    list_files = listdir(file_dir)\n",
    "    list_files = [i for i in list_files if i[0] != '.']\n",
    "    \n",
    "    ## filter files to specified segment\n",
    "    def us_range(x, target_range = segment):\n",
    "        try:\n",
    "            x[0:7]\n",
    "        except:\n",
    "            return False\n",
    "        if x[0:7] in target_range:\n",
    "            return True\n",
    "        else:\n",
    "             return False\n",
    "    list_files = filter(us_range, list_files)\n",
    "    \n",
    "    ## assemble files\n",
    "    all_data = list()\n",
    "    for i in list_files:\n",
    "        file_iter = pd.read_csv(file_dir + '/' + i, parse_dates = ['DATE'],\n",
    "                                dtype = ucl)\n",
    "        \n",
    "        ## deconstruct day/times\n",
    "        file_iter['Day'] = file_iter['DATE'].dt.dayofyear\n",
    "        file_iter['Hour'] = file_iter['DATE'].dt.hour\n",
    "        file_iter = file_iter.drop('DATE', axis = 1)\n",
    "        \n",
    "        ## score weather\n",
    "        file_iter['HourlyPrecipitation'] = file_iter[\n",
    "            'HourlyPrecipitation'].fillna(0)\n",
    "        file_iter['Temperate'] = (file_iter['HourlyDryBulbTemperature'] > 55) &\\\n",
    "            (file_iter['HourlyDryBulbTemperature'] < 75) &\\\n",
    "            (file_iter['HourlyPrecipitation'] < 0.4)\n",
    "        file_iter['Temperate'] = file_iter['Temperate'].astype(int)\n",
    "        file_iter = file_iter.drop(['HourlyDryBulbTemperature',\n",
    "            'HourlyPrecipitation'], axis = 1)\n",
    "        \n",
    "        ## rename columns to make capitalization consistent\n",
    "        file_iter = file_iter.rename(columns = {'LATITUDE':'Lat',\n",
    "            'LONGITUDE': 'Lon', 'ELEVATION':\"Elev\"})\n",
    "        \n",
    "        ## sample data if specified\n",
    "        if sample_fraction < 1:\n",
    "            assert sample_fraction > 0\n",
    "            sample_n = file_iter.shape[0] * sample_fraction\n",
    "            sample_n = int(sample_n)\n",
    "            sample_n = np.max([sample_n, int((0.5**2)/(0.05**2))])\n",
    "            sample_n = np.min([sample_n, file_iter.shape[0]])\n",
    "            file_iter = file_iter.sample(\n",
    "                n = int(sample_n),\n",
    "                weights = (file_iter['Temperate'] * 1) + 1\n",
    "            )\n",
    "\n",
    "        ## drop files outside the US's rough lat/lon box; append others\n",
    "        if max(file_iter.Lat) > 25 and min(file_iter.Lat) < 50:\n",
    "            if max(file_iter.Lon) < -60 and min(file_iter.Lon) > -130:\n",
    "                all_data.append(file_iter)\n",
    "\n",
    "    ## compile files and save\n",
    "    if len(all_data) > 0:\n",
    "        all_data = pd.concat(all_data, axis = 0)\n",
    "        all_data.to_csv('B_Process/model_data/' + str(segment[0]) +\\\n",
    "            '_weather_data.csv.gz', index = False, encoding = 'utf-8')\n",
    "        return all_data\n",
    "    else:\n",
    "        pass\n",
    "        #print('WARNING: ' + str(segment[0]) + ' files contain no valid data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ca0fae-d284-4668-b492-c09354b59035",
   "metadata": {},
   "source": [
    "#### load_model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "457638df-f721-4ce1-b4aa-9b8ce66168cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_data(file_directory = 'B_Process/model_data'):\n",
    "    all_data = listdir(file_directory)\n",
    "    all_data = [i for i in all_data if i[-6:] == 'csv.gz']\n",
    "    for i in range(len(all_data)):\n",
    "        all_data[i] = pd.read_csv(file_directory + '/' + all_data[i])\n",
    "    all_data = pd.concat(all_data, axis = 0)\n",
    "    all_data.reset_index()\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3348c2-11a4-4644-89db-64ddbf6af7db",
   "metadata": {},
   "source": [
    "#### Execute Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "039fa2fb-b1e0-4eae-9cdf-68505a3a5160",
   "metadata": {},
   "outputs": [],
   "source": [
    "if set_gather_data:\n",
    "    for i in valid_prefix:\n",
    "        for j in range(2015, 2020):\n",
    "            segment_iter = [str(j) + '_' + str(i)]\n",
    "            refine_data('B_Process/downloads', segment = segment_iter)\n",
    "\n",
    "model_data = load_model_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad05bf5-0483-4eab-9eec-d5b4031727d2",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df15523d-3a01-4e39-ad65-eabc1732f483",
   "metadata": {},
   "source": [
    "#### split data into train and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a48b9bef-d62d-4efd-9319-8223b911ef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split data into train and test subsets\n",
    "model_data.loc[:, 'Split'] = np.random.binomial(\n",
    "    n = 1, size = (model_data.shape[0],), p = 0.8).astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802df092-2db8-4ff7-9815-7c805d7dcda7",
   "metadata": {},
   "source": [
    "#### model_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d87f2869-fd9a-4ea5-9024-45e0cc22fb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_means(new_data, mod):\n",
    "    new_data = new_data[['Lon', 'Lat', 'Day', 'Hour']]\n",
    "    return mod.predict(new_data)\n",
    "\n",
    "def model_weather(dat = model_data):\n",
    "    \n",
    "    ## split data into train and test data\n",
    "    test_data = dat[~dat['Split']]\n",
    "    dat = dat[dat['Split']]\n",
    "    \n",
    "    ## round off data and average for each rounded grid\n",
    "    simple_dat = dat.drop(['Elev', 'Split'], axis = 1).round()\n",
    "    simple_dat.loc[:, 'Day']  = np.ceil(simple_dat.loc[:, 'Day']  / 5) * 5\n",
    "    simple_dat.loc[:, 'Hour'] = np.ceil(simple_dat.loc[:, 'Hour'] / 2) * 2\n",
    "    simple_dat.loc[:, ['Lon', 'Lat', 'Day', 'Hour']] = simple_dat.astype(int)\n",
    "    simple_dat = simple_dat.groupby(['Lon', 'Lat', 'Day', 'Hour']).mean()\n",
    "    simple_dat = simple_dat.reset_index()\n",
    "    \n",
    "    ## train k nearest neighbor model\n",
    "    knn_model = KNeighborsRegressor(weights = 'distance')\n",
    "    knn_model = knn_model.fit(\n",
    "                                simple_dat[['Lon', 'Lat', 'Day', 'Hour']],\n",
    "                                simple_dat['Temperate']\n",
    "                                )\n",
    "    \n",
    "    ## save model to file\n",
    "    with open('B_Process/knn_model.pkl', 'wb') as conn:\n",
    "        pickle.dump(knn_model, conn)\n",
    "        conn.close()\n",
    "    \n",
    "    ## announce accuracy of predictor function\n",
    "    ml_score = find_closest_means(test_data, knn_model) > 0.5\n",
    "    ml_score = ml_score.astype(int)\n",
    "    ml_score = f1_score(test_data['Temperate'].values, ml_score).round(3)\n",
    "    print('Weather Model F1: ' + str(ml_score) + ' (Threshold = 0.5)')\n",
    "    \n",
    "    return knn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439e9ea6-f04d-43dd-8cf9-517192460ad8",
   "metadata": {},
   "source": [
    "#### Execute Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e60a142-a4fd-4e74-b842-b524faa2ba28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option #1\n"
     ]
    }
   ],
   "source": [
    "if isfile('B_Process/knn_model.pkl') and ~set_gather_data:\n",
    "    with open(\"B_Process/knn_model.pkl\", 'rb') as conn:\n",
    "        weather_model = pickle.load(conn)\n",
    "        conn.close()\n",
    "else:\n",
    "    weather_model = model_weather()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ddf3c4-792a-4f0d-90a7-94d631ffb412",
   "metadata": {},
   "source": [
    "# Model Routes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf6fec9-7726-4317-a363-aafa35c12812",
   "metadata": {},
   "source": [
    "#### model_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7495db0f-0a5d-4124-ab73-dd0ac693c0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5055898290579098"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_city(pd_nxy, mod):\n",
    "    \n",
    "    ## warn if outside range\n",
    "    geo_bound_check = [\n",
    "        pd_nxy.Lon < max(set_geo_box['Lon']),\n",
    "        pd_nxy.Lon > min(set_geo_box['Lon']),\n",
    "        pd_nxy.Lat < max(set_geo_box['Lat']),\n",
    "        pd_nxy.Lat > min(set_geo_box['Lat'])\n",
    "        ]\n",
    "    \n",
    "    if all(geo_bound_check):\n",
    "        pass\n",
    "    else:\n",
    "        print('WARNING: ' + pd_nxy.City + ' is outside the geographic' +\\\n",
    "             ' bounds used to create the model.  Results may be inaccurate.\\n')\n",
    "    \n",
    "    ## expand data to include all days of the year and hours of the day\n",
    "    full_data = np.meshgrid(np.arange(1, 366), np.arange(0, 24))\n",
    "    full_data = pd.DataFrame(map(np.ravel, full_data)).T\n",
    "    full_data.columns = ['Day', 'Hour']\n",
    "    for i in pd_nxy.index:\n",
    "        full_data[i] = pd_nxy[i]\n",
    "    \n",
    "    ## predict weather\n",
    "    full_data['Temperate'] = find_closest_means(full_data, mod)\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db202356-cbed-45ff-8209-39da146a8910",
   "metadata": {},
   "source": [
    "#### model_routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "81139a5e-7c88-470c-8629-c1dac88e1677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_route(route, cl = city_list, mod = weather_model):\n",
    "    \n",
    "    ## extract coordinates of cities on the route\n",
    "    pd_nxy = cl.loc[cl.Route == route, ['City', 'Lon', 'Lat']]\n",
    "    \n",
    "    ## predict weather conditions for each set of coordinates\n",
    "    route_weather = []\n",
    "    for i in range(pd_nxy.shape[0]):\n",
    "        city_iter = pd_nxy.iloc[i,:]\n",
    "        route_weather.append(model_city(city_iter, mod))\n",
    "    route_weather = pd.concat(route_weather, axis = 0)\n",
    "    \n",
    "    ## score each day for temperate weather across routes\n",
    "    i = route_weather.Hour\n",
    "    route_weather = route_weather[(i > 7) & (i < 23)]\n",
    "    route_weather = route_weather.drop(['Lon', 'Lat', 'City', 'Hour'], axis = 1)\n",
    "    route_weather = route_weather.groupby('Day').mean().round(2)\n",
    "    route_weather = route_weather.reset_index()\n",
    "\n",
    "    ## inject route name into dataset and reorder\n",
    "    route_weather['Route'] = route\n",
    "    i = ['Route', 'Day', 'Temperate']\n",
    "    route_weather = route_weather[i]\n",
    "    \n",
    "    return route_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07ff15f-d7de-4e03-a671-ec16c717f0d3",
   "metadata": {},
   "source": [
    "#### find_best_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9f5e10ea-e144-4969-8c06-e612d5d8f37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Route  Day  Temperate\n",
      "0    California Plus    1       0.48\n",
      "1    California Plus    2       0.48\n",
      "2    California Plus    3       0.48\n",
      "3    California Plus    4       0.48\n",
      "4    California Plus    5       0.49\n",
      "..               ...  ...        ...\n",
      "360    Florida State  361       0.69\n",
      "361    Florida State  362       0.69\n",
      "362    Florida State  363       0.57\n",
      "363    Florida State  364       0.57\n",
      "364    Florida State  365       0.57\n",
      "\n",
      "[5475 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def find_best_month(rm):\n",
    "    \n",
    "    ## calculate moving 30 day averages\n",
    "    \n",
    "    ## find best 30d period in each half of the year for each roupte\n",
    "    rm['Half'] = rm['Day'] < 182\n",
    "    #rm['Best'] = \n",
    "    \n",
    "    print(rm)\n",
    "\n",
    "find_best_month(routes_modeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa8d627-37a4-4bc6-840d-f0047c226001",
   "metadata": {},
   "source": [
    "#### execute code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a240c772-5eb2-4d78-8c52-094ead40b4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Anchorage AK is outside the geographic bounds used to create the model.  Results may be inaccurate.\n",
      "\n",
      "WARNING: Fairbanks AK is outside the geographic bounds used to create the model.  Results may be inaccurate.\n",
      "\n",
      "WARNING: Honolulu HI is outside the geographic bounds used to create the model.  Results may be inaccurate.\n",
      "\n",
      "WARNING: San Juan PR is outside the geographic bounds used to create the model.  Results may be inaccurate.\n",
      "\n",
      "WARNING: Key West FL is outside the geographic bounds used to create the model.  Results may be inaccurate.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Score average temperateness for all cities across each route\n",
    "routes_modeled = []\n",
    "for i in set(city_list.Route):\n",
    "    routes_modeled.append(model_route(i))\n",
    "routes_modeled = pd.concat(routes_modeled)\n",
    "\n",
    "## determine the best 30 days routes for each route in each half of the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f70fea5-2cb1-4943-9891-ffd39912e048",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spare parts (approach will be folded into the new find_best_month function)\n",
    "if False:\n",
    "    ## score all 30 ranges of days for temperate weather\n",
    "    route_weather['MovingMean30d'] = np.nan\n",
    "    for i in np.arange(1, 366):\n",
    "        j = np.arange(i - 15, i + 16) % 365\n",
    "        j = route_weather.Day.isin(j)\n",
    "        j = route_weather.loc[j, 'Temperate'].mean()\n",
    "        route_weather.loc[i - 1, 'MovingMean30d'] = j\n",
    "    \n",
    "    ## find best 30 range in each half of the year\n",
    "    route_weather['Best30d'] = route_weather.Day >= 182\n",
    "    route_weather.Best30d = route_weather.groupby('Best30d').MovingMean30d\\\n",
    "        .transform(max)\n",
    "    route_weather.Best30d = route_weather.Best30d == route_weather.MovingMean30d\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a337981f-da13-4141-a926-1d95230b30a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24382e0-9187-4470-8e3b-6969112a5d6e",
   "metadata": {},
   "source": [
    "# Render Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7d149be-a778-4c5b-b5e6-40ef188d6c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Check: Point Z\n",
      "2022-03-04 09:43:21.591557\n"
     ]
    }
   ],
   "source": [
    "## TODO: centralize geographic boundary box setting\n",
    "time_check('Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c427544a-6ef2-440b-8049-4ca5c0b96b68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
