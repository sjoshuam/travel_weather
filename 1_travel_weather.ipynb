{
 "cells": [
  {
   "cell_type": "raw",
   "id": "78d5d60d-1af9-4e44-87cb-3a6b3dd18e18",
   "metadata": {},
   "source": [
    "0. Import Data – Import all needed data.  This is a large task.\n",
    "1. Refine Data – Clean and reshape data into an enriched form.\n",
    "2. Build Model – Trained a model to predict the weather at destinations.\n",
    "3. Model Routes – Predict weather at destinations along the route.\n",
    "4. Render Visualization – Visualize weather temperateness for each route.\n",
    "5. Test Code - Tests for all features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778be4aa-e671-4279-a6cc-24ec56d88ced",
   "metadata": {},
   "source": [
    "# Libraries and Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb2a9ee-47c8-4e1f-86ed-e90e8c79e8dc",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96bfe8c1-b00a-45c5-b622-242563a6a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########==========\n",
    "\n",
    "## general purpose libraries\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request    as url\n",
    "\n",
    "from datetime          import datetime, timedelta\n",
    "from bs4               import BeautifulSoup\n",
    "from os                import listdir, mkdir\n",
    "from os.path           import isdir, isfile\n",
    "from geopy.distance    import distance\n",
    "from docx              import Document\n",
    "from textwrap          import TextWrapper\n",
    "from matplotlib.dates  import DateFormatter\n",
    "from matplotlib.colors import hsv_to_rgb, to_hex\n",
    "\n",
    "\n",
    "## time stamp functions\n",
    "set_timing = dict()\n",
    "def time_check(id_str = 'ZZ'):\n",
    "    raw_time = datetime.now()\n",
    "    current_time = [raw_time.hour, raw_time.minute, round(raw_time.second)]\n",
    "    current_time = [str(i).zfill(2) for i in current_time]\n",
    "    current_time = ':'.join(current_time)\n",
    "    current_time = 'Time Check ' + id_str.ljust(4) + ' = ' + current_time\n",
    "    set_timing[id_str] = current_time\n",
    "    if id_str == 'ZZ':\n",
    "        for i in set_timing:\n",
    "            print(set_timing[i])\n",
    "\n",
    "time_check('AA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b536436a-2dd9-435b-9561-037d68df7480",
   "metadata": {},
   "source": [
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ab7de13-3263-431a-8de4-6c12ce36c5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## url source of the data\n",
    "set_url = \"https://www.ncei.noaa.gov/data/local-climatological-data/access/{0}/\"\n",
    "set_year = range(2012, 2022)\n",
    "set_prefix = ['69', '70', '72', '74', '91', '99']#.append('71') # 71 = Canada\n",
    "\n",
    "## cache settings\n",
    "set_cache = {'hard_reset':     False,\n",
    "             'station_roster': True,  # wired\n",
    "             'collection':     True,  # wired\n",
    "             'compilation':    True, # wired\n",
    "             'modeling':       False}\n",
    "\n",
    "## defines dtype for the columns in the raw weather data files\n",
    "set_col_list = {'DATE':str, 'LATITUDE':float,\n",
    "    'LONGITUDE':float, #'ELEVATION':float,\n",
    "    \"HourlyDryBulbTemperature\":float,\n",
    "    \"HourlyPrecipitation\":float}\n",
    "\n",
    "## set modeling and display parameters\n",
    "set_param = {'home_city': 'Washington DC'}\n",
    "\n",
    "set_color = [1/12, 4/12, 7/12, 10/12]\n",
    "set_color = [hsv_to_rgb((i, 0.7, 0.7)) for i in set_color]\n",
    "set_color = [\n",
    "    (7/12, 0.7, 0.7), (1/12, 0.5, 0.9), (1/12, 0.7, 0.7), (7/12, 0.5, 0.9)]\n",
    "set_color = [to_hex(hsv_to_rgb(i)) for i in set_color]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d630bfdf-3734-4605-87d0-0e5671ac35d3",
   "metadata": {},
   "source": [
    "#### set up file system (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c478cc19-470a-4b9e-8f12-19da7a63fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make directories that script expects\n",
    "def make_directories():\n",
    "    all_dirs = ['A_Input', 'B_Process', 'C_Output']\n",
    "    all_dirs = all_dirs + ['B_Process/downloads']\n",
    "    for i in all_dirs:\n",
    "        if not isdir(i): mkdir(i)\n",
    "        \n",
    "make_directories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1836ea5b-e330-4de0-bb0c-a7dd64d6a097",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf07b37-777b-4ae9-b591-d72056be7f19",
   "metadata": {},
   "source": [
    "#### retrieve city roster file and text explanation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92f2f34b-bca8-4a2d-8f32-e8e934ae9c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read/format project explanation docx\n",
    "def read_explainer(address = 'A_Input/explanation.docx', txt_len = 50):\n",
    "    \n",
    "    ## read in file\n",
    "    doc = Document(address).paragraphs\n",
    "    doc = [i.text for i in doc]\n",
    "    doc = ' '.join(doc)\n",
    "    \n",
    "    ## wrap text\n",
    "    tw = TextWrapper(width = txt_len, \n",
    "                     replace_whitespace = False, fix_sentence_endings = True,\n",
    "                     break_long_words = False, drop_whitespace = False)\n",
    "    doc = tw.fill(doc)\n",
    "    doc = doc.format('\\n\\n')\n",
    "    return doc\n",
    "    \n",
    "## execute functions\n",
    "city_roster = pd.read_excel('A_Input/city_list.xlsx',\n",
    "    usecols = {'City': str, 'Route': str, 'lon': float, 'lat': float})\n",
    "## project_explainer = read_explainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108940fa-42b4-46cf-b7e6-8b317617cf52",
   "metadata": {},
   "source": [
    "#### retrieve links page for first year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "490866ba-715d-4ded-887d-756871df8d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_roster(the_year, the_url = set_url, valid_prefix = set_prefix):\n",
    "  \n",
    "    ## retrive raw web page\n",
    "    the_url = the_url.format(the_year)\n",
    "    with url.urlopen(the_url) as conn:\n",
    "        all_files = conn.read()\n",
    "        conn.close()\n",
    "        \n",
    "    ## extract links to csv files\n",
    "    all_files = BeautifulSoup(all_files)\n",
    "    all_files = all_files.find_all('a')\n",
    "    all_files = [i.string for i in all_files if i.string.find('.csv') != -1]\n",
    "        \n",
    "    ## package as pandas files; limit to valid prefix\n",
    "    all_files = pd.DataFrame({'prefix':0, 'file':all_files,\n",
    "        'lon': np.nan, 'lat': np.nan, 'dist': np.nan, 'city': 'Unknown'})\n",
    "    all_files['prefix'] = [i[0:2] for i in all_files['file']]\n",
    "    all_files = all_files[all_files.prefix.isin(valid_prefix)]\n",
    "    all_files = all_files.reset_index()\n",
    "\n",
    "    del all_files['index']\n",
    "    \n",
    "    return all_files \n",
    "\n",
    "## execute code\n",
    "data_roster = make_data_roster(set_year[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089d7862-e61e-48a3-812c-5aef172d8dbf",
   "metadata": {},
   "source": [
    "#### determine which stations are near route cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d0b283f-24fe-49ef-b186-ba4fb739fe09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache or make: Retrieving cache\n"
     ]
    }
   ],
   "source": [
    "def read_station_data(x, ucl = set_col_list):\n",
    "    the_csv = pd.read_csv(x, usecols = ucl, parse_dates = ['DATE'], dtype = str)\n",
    "    for j in ucl.keys():\n",
    "        if ucl[j] == float:\n",
    "            the_csv[j] = pd.to_numeric(the_csv[j], errors = 'coerce')\n",
    "    the_csv.LATITUDE = the_csv.LATITUDE.fillna(0)\n",
    "    the_csv.LONGITUDE = the_csv.LONGITUDE.fillna(0)\n",
    "    return the_csv\n",
    "\n",
    "def filter_to_proximate_data(\n",
    "    dat_rost = data_roster, the_url = set_url, city = city_roster):\n",
    "\n",
    "    print('Evaluating:', end = ' ')\n",
    "    for i in dat_rost.index:\n",
    "        ## print(dat_rost.loc[i, 'file'], end=', ')\n",
    "        \n",
    "        ## download file and coerce numeric columns to numeric\n",
    "        full_url = set_url.format(set_year[-1]) + dat_rost.loc[i, 'file']\n",
    "        the_csv = read_station_data(full_url)\n",
    "\n",
    "        ## find distance to closest rostered metro area\n",
    "        min_dist = 999999\n",
    "        prox_city = None\n",
    "        for j in city.index:\n",
    "            dist_j = distance(\n",
    "                tuple(the_csv.loc[0, ['LATITUDE', 'LONGITUDE']]),\n",
    "                tuple(city.loc[j, ['lat', 'lon']])).miles\n",
    "            if dist_j < min_dist:\n",
    "                min_dist = dist_j\n",
    "                prox_city = city.loc[j, 'City']\n",
    "        dat_rost.loc[i, 'dist'] = np.round(min_dist)\n",
    "        dat_rost.loc[i, 'city'] = prox_city\n",
    "        \n",
    "    ## save roster file to disk\n",
    "    dat_rost.to_csv('B_Process/station_roster.csv')\n",
    "    return dat_rost\n",
    "\n",
    "def cache_or_make_csv(address, cache_bool, function):\n",
    "    if not cache_bool or not isfile(address):\n",
    "        print('Cache or make: Making file')\n",
    "        x = function()\n",
    "    else:\n",
    "        print('Cache or make: Retrieving cache')\n",
    "        x = pd.read_csv(address)\n",
    "    return x\n",
    "\n",
    "## execution functions\n",
    "station_roster = cache_or_make_csv(\n",
    "    address = 'B_Process/station_roster.csv',\n",
    "    cache_bool = set_cache['station_roster'],\n",
    "    function = filter_to_proximate_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2922e14-8c66-455f-8ba3-ae2aa89c15e4",
   "metadata": {},
   "source": [
    "#### retrieve data from subsequent years for proximate stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2629af6-0762-432a-8b66-4c2f57e4722b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO: Evaluate whether to build an override that forces new download\n"
     ]
    }
   ],
   "source": [
    "def retrieve_station_data(\n",
    "    roster = station_roster, url = set_url, year = set_year, ucl = set_col_list):\n",
    "    \n",
    "    ## download all relavent data files\n",
    "    for i in year:\n",
    "        if not str(i) in roster.columns: roster[str(i)] = False\n",
    "        needed_file = (roster.dist < 30) & ~roster[str(i)]\n",
    "        for j in roster.index[needed_file]:\n",
    "            ## download data file\n",
    "            try:\n",
    "                error_flag = False\n",
    "                x = read_station_data(url.format(i) + roster.loc[j, 'file'])\n",
    "                if i == year[0]:\n",
    "                    roster.loc[j, 'lon'] = x.loc[0, 'LONGITUDE']\n",
    "                    roster.loc[j, 'lat'] = x.loc[0, 'LATITUDE']\n",
    "                x.to_csv(\n",
    "                    'B_Process/downloads/' + str(i) + '_' +\n",
    "                    roster.loc[j, 'file'] + '.gz')\n",
    "            except:\n",
    "                error_flag = True\n",
    "            \n",
    "            ## update roster\n",
    "            if error_flag: roster.loc[j, str(i)] = False\n",
    "            else: roster.loc[j, str(i)] = True\n",
    "            roster.to_csv('B_Process/station_roster.csv')\n",
    "\n",
    "## execution functions\n",
    "if not set_cache['collection']: retrieve_station_data()\n",
    "print('TODO: Evaluate whether to build an override that forces new download')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74bd876-4fc2-45d1-a41d-6e23cb75fb63",
   "metadata": {},
   "source": [
    "#### record completion time stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec88b623-7352-45ed-addd-3632dac6f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_check('ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee54b404-eaa6-4743-8181-caced638143c",
   "metadata": {},
   "source": [
    "# Refine Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba7125-6a8f-45f8-979f-a4cc05a75443",
   "metadata": {},
   "source": [
    "#### Refine and combine files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f953b7ff-3b44-4ad2-83ea-94cb7fa6e6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache or make: Retrieving cache\n"
     ]
    }
   ],
   "source": [
    "def compile_weather_data(address = 'B_Process/downloads', ucl = set_col_list):\n",
    "    \n",
    "    ## make list of all available data files\n",
    "    file_list = listdir(address)\n",
    "    file_list = filter(lambda x: x[-7::] == '.csv.gz', file_list)\n",
    "    \n",
    "    ## read in the data\n",
    "    weather_data = []\n",
    "    for i in file_list:\n",
    "        \n",
    "        ## read in file\n",
    "        x = read_station_data('B_Process/downloads/' + i)\n",
    "        x['source'] = i\n",
    "        \n",
    "        ## unpack date information\n",
    "        x['hour'] = x.DATE.dt.hour\n",
    "        x['day'] = x.DATE.dt.dayofyear\n",
    "        x = x.drop('DATE', axis = 1)\n",
    "        x = x.loc[x.day != 366, ]\n",
    "        \n",
    "        ## score weather\n",
    "        for i in ['HourlyPrecipitation', 'HourlyDryBulbTemperature']:\n",
    "            x[i] = x[i].fillna(0)\n",
    "        x['score'] = (x['HourlyDryBulbTemperature'] <= 75) &\\\n",
    "            (x['HourlyDryBulbTemperature'] >= 55) &\\\n",
    "            (x['HourlyPrecipitation'] <= 0.2)\n",
    "        x['score'] = x['score'].astype(int)\n",
    "        x = x.drop(['HourlyDryBulbTemperature', 'HourlyPrecipitation'], axis = 1)\n",
    "        \n",
    "        ## add refined file to data list\n",
    "        weather_data.append(x)\n",
    "        \n",
    "    ## compile as dataframe and save to disk\n",
    "    weather_data = pd.concat(weather_data, axis = 0)\n",
    "    weather_data.to_csv('B_Process/weather_data.csv.gz')\n",
    "    \n",
    "    return weather_data\n",
    "\n",
    "## execute function\n",
    "weather_data = cache_or_make_csv(\n",
    "    address = 'B_Process/weather_data.csv.gz',\n",
    "    cache_bool = set_cache['compilation'],\n",
    "    function = compile_weather_data\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b858194d-9ad8-4823-b5cf-05b8ebb7c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_check('RD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0366c932-ce0d-44da-aa7f-8cda8c66c8fb",
   "metadata": {},
   "source": [
    "# Model Routes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79616312-23ba-4db0-809f-45d9974779d3",
   "metadata": {},
   "source": [
    "#### summarize weather score for each city and then for each route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda8671e-003c-4bcd-9826-efe798ab4bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## score average temperateness of weather for each city\n",
    "def summarize_by_city(wd = weather_data, sr = station_roster):\n",
    "    \n",
    "    ## assign stations to closest metropolitan area\n",
    "    wd['index'] = [i[5:-3] for i in wd.source]\n",
    "    wd = wd.set_index('index')\n",
    "    sr = sr.set_index('file')\n",
    "    wd = wd.join(sr[['city']])\n",
    "    \n",
    "    ## calculate average weather scores\n",
    "    wd = wd[['city', 'day', 'hour', 'score']]\n",
    "    wd = wd.groupby(['city', 'day', 'hour']).mean().reset_index()\n",
    "    wd['score'] = wd['score'].round(6)\n",
    "    \n",
    "    wd.to_csv('B_Process/city_score.csv')\n",
    "    return wd\n",
    "\n",
    "## calculate average temperateness score across cities in each route\n",
    "def summarize_by_route(cs, cr = city_roster):\n",
    "    \n",
    "    ## merge route assignments\n",
    "    cs = cs.set_index('city')\n",
    "    cr = cr.set_index('City')\n",
    "    cs = cs.join(cr[['Route']])\n",
    "    \n",
    "    ## average scores by route\n",
    "    cs = cs.groupby(['Route', 'day', 'hour']).mean().reset_index()\n",
    "    cs['score'] = cs['score'].round(6)\n",
    "    cs.to_csv('B_Process/route_score.csv')\n",
    "    return cs\n",
    "\n",
    "## add dc to route scores\n",
    "def add_home_city(rs, cs, hc):\n",
    "    hc = cs.loc[cs.city == hc, ]\n",
    "    hc.columns = rs.columns\n",
    "    rs = pd.concat([rs, hc], axis = 0)\n",
    "    return rs\n",
    "\n",
    "## execute code\n",
    "city_score  = summarize_by_city()\n",
    "route_score = summarize_by_route(cs = city_score)\n",
    "route_score = add_home_city(route_score, city_score, set_param['home_city'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669ec06b-626b-43fc-a14f-e126809dc11a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### calculate daily summary scores (temperate hours and best time per day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebad49e-1668-43e0-a0a7-33f508c5cb3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## sum hours of good weather per day\n",
    "def sum_temperate_by_day(rs = route_score):\n",
    "    \n",
    "    ## sum number of temperate hours by day\n",
    "    rs = rs.loc[(rs.hour > 6) & (rs.hour < 20), ]\n",
    "    hour_sum = rs.drop('hour', axis = 1).groupby(['Route', 'day']).sum()\n",
    "    hour_sum = hour_sum.round().astype(int).reset_index()\n",
    "    \n",
    "    return hour_sum\n",
    "\n",
    "## calculate best travel months\n",
    "def median_temperate_by_month(ds):\n",
    "    \n",
    "    ## convert day of the year into month\n",
    "    ds['month'] = datetime.fromisoformat('2000-01-01')\n",
    "    for i in ds.index:\n",
    "        ds.loc[i, 'month'] = ds.loc[i, 'month'] +\\\n",
    "            timedelta(days = int(ds.loc[i, 'day']))\n",
    "        ds.loc[i, 'month'] = ds.loc[i, 'month'].month\n",
    "        \n",
    "    ## sum hours by month and reshape\n",
    "    ds = ds.groupby(['Route','month']).median().drop('day', axis = 1)\n",
    "    ds = ds.reset_index().pivot(\n",
    "        index = 'Route', columns = 'month', values = 'score').astype(int)\n",
    "    \n",
    "    ## categorize routes on a summer to winter scale\n",
    "    route_weather = np.round(np.array(ds) / 12, 1)\n",
    "    winter_score = np.ones(route_weather.shape) * np.abs(np.arange(1, 13) - 7)\n",
    "    winter_score = winter_score + 1\n",
    "    route_class = (route_weather * winter_score).sum(axis = 1)\n",
    "    route_class = route_class / route_weather.sum(axis = 1)\n",
    "    ds['weather_class'] = np.round(route_class, 1)\n",
    "    ds = ds.sort_values('weather_class')\n",
    "    ds['weather_class'] = np.arange(0, ds.shape[0]) / (ds.shape[0] / 4)\n",
    "    ds['weather_class'] = ds['weather_class'].astype(int)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "## execute code\n",
    "day_score = sum_temperate_by_day()\n",
    "month_score = median_temperate_by_month(day_score.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2887dbc7-fa59-4dc0-9f2e-4a58e5119c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time_check('MR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b220a9-e4c8-4498-8219-7855ba983845",
   "metadata": {},
   "source": [
    "# Render Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823dc61e-d5ed-43ad-9bf2-78be57e0a45a",
   "metadata": {},
   "source": [
    "#### unpack data into suitable format for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699223da-1815-43b1-b659-18695f55a746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_around_rolling(x, w = 30):\n",
    "    y = pd.concat([x[len(x) - w + 1::], x]).rolling(w, center = True)\n",
    "    return y.mean().dropna()\n",
    "\n",
    "def unpack_data(ds = day_score):\n",
    "    \n",
    "    ## unpack routes to separate list objects\n",
    "    unpacked = dict()\n",
    "    for i in set(ds.Route):\n",
    "        unpacked_iter = ds.loc[ds.Route == i, :].copy()\n",
    "        \n",
    "        ## calculate 30 moving average of scores for visualization purposes\n",
    "        unpacked_iter['mov_av'] = wrap_around_rolling(unpacked_iter.score)\n",
    "        \n",
    "        ## calculate dates and adjust by 15 days (so date is the center of M.A.)\n",
    "        f = lambda x: datetime.fromisoformat('2001-01-01') + timedelta(days = x)\n",
    "        unpacked_iter['date'] = unpacked_iter['day'].apply(f)\n",
    "        \n",
    "        ## append route data to object\n",
    "        unpacked[i] = unpacked_iter.sort_values('date')\n",
    "    \n",
    "    return unpacked\n",
    "\n",
    "visual_data = unpack_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508d734d-741a-4a0d-9714-20a850903f23",
   "metadata": {},
   "source": [
    "#### Set up plot infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80be7749-aa60-44a0-bff3-8d46a59134c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_layout():\n",
    "    \n",
    "    ## set font\n",
    "    plt.rcParams['font.sans-serif'] = ['Quicksand']\n",
    "    \n",
    "    ## make figure\n",
    "    dash_figure = plt.figure(figsize = (9, 6))\n",
    "    dash_grid = dash_figure.add_gridspec(2, 3, figure = dash_figure,\n",
    "        hspace = 0, wspace = 0,\n",
    "        bottom = 0.07, top = 1 - 0.05,\n",
    "        left = 0.05 * (6/9),  right = 1 - 0.05 * (6/9)\n",
    "        )\n",
    "    dash_figure.set_facecolor('white')\n",
    "    \n",
    "    ## make axes\n",
    "    dash_axis = dict()\n",
    "    dash_axis['2']   = dash_figure.add_subplot(dash_grid[1, 0])\n",
    "    dash_axis['1']   = dash_figure.add_subplot(dash_grid[1, 1])\n",
    "    dash_axis['0']   = dash_figure.add_subplot(dash_grid[1, 2])\n",
    "    dash_axis['mat'] = dash_figure.add_subplot(dash_grid[0, 1:3])\n",
    "    dash_axis['3']   = dash_figure.add_subplot(dash_grid[0, 0])\n",
    "    \n",
    "    ## all figures\n",
    "    for i in dash_axis.keys():\n",
    "        dash_axis[i].set_facecolor('white')\n",
    "        dash_axis[i].tick_params(left = False, bottom = False,\n",
    "            labelleft = False, labelbottom = False,\n",
    "            top = False, labeltop = False,\n",
    "            right = False, labelright = False\n",
    "                                )\n",
    "        \n",
    "    ## set x axis\n",
    "    for i in ['0', '1', '2']:\n",
    "        dash_axis[i].tick_params(labelbottom = True, bottom = True)\n",
    "        \n",
    "    ## set y axis\n",
    "    for i in ['2', '3']:\n",
    "        dash_axis[i].tick_params(labelleft = True, left = True)\n",
    "        \n",
    "    ## weather line plots\n",
    "    date_format = DateFormatter('%b')\n",
    "    for i in range(0, 4):\n",
    "        dash_axis[str(i)].set_xlim(datetime.fromisoformat('2001-01-01'),\n",
    "            datetime.fromisoformat('2001-12-31'))\n",
    "        dash_axis[str(i)].set_ylim(0, 13)\n",
    "        dash_axis[str(i)].xaxis.set_major_formatter(date_format)\n",
    "        dash_axis[str(i)].tick_params(axis='x', labelrotation = 90)\n",
    "\n",
    "    return dash_figure, dash_axis\n",
    "    \n",
    "##\n",
    "dash_figure, dash_axis = make_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99569707-6734-459a-bb10-5eedc18f5af6",
   "metadata": {},
   "source": [
    "#### route score panels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb803816-1db1-48f3-bdd1-c99ce5916396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_weather_lines(ms = month_score, vd = visual_data):\n",
    "\n",
    "    color_iter = 0\n",
    "    for i in range(0, 4):\n",
    "        \n",
    "        ## draw month guides\n",
    "        x = ['2001-03-01', '2001-06-01', '2001-09-01', '2001-12-01']\n",
    "        x = [datetime.fromisoformat(i) for i in x]\n",
    "        dash_axis[str(i)].vlines(x = x, ymin = 0, ymax = 13, color = '#F0F0F0')\n",
    "        dash_axis[str(i)].hlines(y = [4, 8], color = '#F0F0F0',\n",
    "            xmin = datetime.fromisoformat('2001-01-01'),\n",
    "            xmax = datetime.fromisoformat('2001-12-31')\n",
    "            )\n",
    "        \n",
    "        ## generate short labels\n",
    "        short_labels = dict()\n",
    "        for k in set(ms.index):\n",
    "            if len(k) > 8:\n",
    "                short_labels[k] = k[0:8].rstrip() + '.'\n",
    "            else:\n",
    "                short_labels[k] = k\n",
    "        \n",
    "        ## draw weather score lines\n",
    "        for j in ms.index[ms.weather_class == i]:\n",
    "            dash_axis[str(i)].plot(vd[j].date, vd[j].mov_av,\n",
    "                label = short_labels[j], color = set_color[color_iter])\n",
    "            color_iter = (color_iter + 1) % len(set_color)\n",
    "            \n",
    "            dash_axis[str(i)].legend(loc = 'upper center', ncol = 2,\n",
    "                labelspacing = 0, borderpad = 0.1, borderaxespad = 0.2)\n",
    "\n",
    "render_weather_lines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d768839-cdd6-4f58-98ea-cefb6b580aba",
   "metadata": {},
   "source": [
    "#### matrix panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931f7c39-0ae7-460f-81bf-d3df680224c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_hours_table(ms):\n",
    "    \n",
    "    ## convert table to matplotlib's table format\n",
    "    ms = ms.drop(['weather_class'], axis = 1)\n",
    "    ms_list = [list(ms.loc[i, ]) for i in ms.index] \n",
    "    \n",
    "    ## generate cell colors\n",
    "    ms_color = ms\n",
    "    ms_color = (ms_color / 4).astype(int).replace({\n",
    "        0: to_hex(hsv_to_rgb((7/12, 0.0, 1.0))),\n",
    "        1: to_hex(hsv_to_rgb((7/12, 0.2, 0.9))),\n",
    "        2: to_hex(hsv_to_rgb((7/12, 0.4, 0.8)))\n",
    "    })\n",
    "    ms_color = [list(ms_color.loc[i, ]) for i in ms_color.index]\n",
    "    \n",
    "    ## assemble table\n",
    "    dash_axis['mat'].set_xlim(0, 6)\n",
    "    dash_axis['mat'].set_ylim(0, 3)\n",
    "    access_object = dash_axis['mat'].table(\n",
    "        cellText = ms_list,\n",
    "        cellColours = ms_color, cellLoc = 'center', \n",
    "        rowLabels = ms.index, rowLoc = 'right',\n",
    "        colLabels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug',\n",
    "                    'Sep', 'Oct', 'Nov', 'Dec'],\n",
    "        colLoc = 'center',\n",
    "        loc = 'center',\n",
    "        bbox = [0.25, 0.0, 0.77, 0.90]\n",
    "        )\n",
    "    for i, j in access_object.get_celld().items():\n",
    "        j.set_linewidth(0)\n",
    "    access_object.auto_set_font_size(False)\n",
    "    access_object.set_fontsize(9)\n",
    "    \n",
    "    ## add table\n",
    "    dash_axis['mat'].text(\n",
    "        x = 6 - 0.02, y = 3 - 0.02,\n",
    "        s = 'Average Daily Hours of Temperate Weather On Each Travel Route',\n",
    "        horizontalalignment = 'right', verticalalignment = 'top',\n",
    "        fontsize = 12\n",
    "    )\n",
    "\n",
    "render_hours_table(month_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e33d0c0-8ef7-4224-8922-537706338c14",
   "metadata": {},
   "source": [
    "#### save figure to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5768f822-601c-4667-95fb-5d6042e87675",
   "metadata": {},
   "outputs": [],
   "source": [
    "dash_figure.savefig('C_Output/travel_weather.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1f5c76-8a6a-4edb-afa2-6e6302713e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_check('RV')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23023019-0f8e-4cc7-b266-ccb6fe20f7b1",
   "metadata": {},
   "source": [
    "# Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9c6ff3-02a3-4b1d-91e9-acfc85b54b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_check('TC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de2aab-a8a1-4dff-bd0a-bd051ca73d52",
   "metadata": {},
   "source": [
    "# Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a350ac31-ae36-4b98-9865-a387f256cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display time statistics\n",
    "time_check('ZZ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
