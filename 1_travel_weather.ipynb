{
 "cells": [
  {
   "cell_type": "raw",
   "id": "78d5d60d-1af9-4e44-87cb-3a6b3dd18e18",
   "metadata": {},
   "source": [
    "0. Import Data – Import all needed data.  This is a large task.\n",
    "1. Refine Data – Clean and reshape data into an enriched form.\n",
    "2. Build Model – Trained a model to predict the weather at destinations.\n",
    "3. Model Routes – Predict weather at destinations along the route.\n",
    "4. Render Visualization – Visualize weather temperateness for each route.\n",
    "5. Test Code - Tests for all features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778be4aa-e671-4279-a6cc-24ec56d88ced",
   "metadata": {},
   "source": [
    "# Libraries and Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb2a9ee-47c8-4e1f-86ed-e90e8c79e8dc",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "96bfe8c1-b00a-45c5-b622-242563a6a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########==========\n",
    "\n",
    "## general purpose libraries\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request    as url\n",
    "\n",
    "from datetime       import datetime, timedelta\n",
    "from bs4            import BeautifulSoup\n",
    "from os             import listdir, mkdir\n",
    "from os.path        import isdir, isfile\n",
    "from geopy.distance import distance\n",
    "from docx           import Document\n",
    "from textwrap       import TextWrapper\n",
    "\n",
    "\n",
    "## time stamp functions\n",
    "set_timing = dict()\n",
    "def time_check(id_str = 'ZZ'):\n",
    "    raw_time = datetime.now()\n",
    "    current_time = [raw_time.hour, raw_time.minute, round(raw_time.second)]\n",
    "    current_time = [str(i).zfill(2) for i in current_time]\n",
    "    current_time = ':'.join(current_time)\n",
    "    current_time = 'Time Check ' + id_str.ljust(4) + ' = ' + current_time\n",
    "    set_timing[id_str] = current_time\n",
    "    if id_str == 'ZZ':\n",
    "        for i in set_timing:\n",
    "            print(set_timing[i])\n",
    "\n",
    "time_check('AA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b536436a-2dd9-435b-9561-037d68df7480",
   "metadata": {},
   "source": [
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2ab7de13-3263-431a-8de4-6c12ce36c5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## url source of the data\n",
    "set_url = \"https://www.ncei.noaa.gov/data/local-climatological-data/access/{0}/\"\n",
    "set_year = range(2012, 2022)\n",
    "set_prefix = ['69', '70', '72', '74', '91', '99']#.append('71') # 71 = Canada\n",
    "\n",
    "## cache settings\n",
    "set_cache = {'hard_reset':     False,\n",
    "             'station_roster': True,  # wired\n",
    "             'collection':     True,  # wired\n",
    "             'compilation':    True, # wired\n",
    "             'modeling':       False}\n",
    "\n",
    "## defines dtype for the columns in the raw weather data files\n",
    "set_col_list = {'DATE':str, 'LATITUDE':float,\n",
    "    'LONGITUDE':float, #'ELEVATION':float,\n",
    "    \"HourlyDryBulbTemperature\":float,\n",
    "    \"HourlyPrecipitation\":float}\n",
    "\n",
    "## set modeling parameters\n",
    "set_param = {'window': 14, 'mid_summer': 213, 'home_city': 'Washington DC'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d630bfdf-3734-4605-87d0-0e5671ac35d3",
   "metadata": {},
   "source": [
    "#### set up file system (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c478cc19-470a-4b9e-8f12-19da7a63fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make directories that script expects\n",
    "def make_directories():\n",
    "    all_dirs = ['A_Input', 'B_Process', 'C_Output']\n",
    "    all_dirs = all_dirs + ['B_Process/downloads']\n",
    "    for i in all_dirs:\n",
    "        if not isdir(i): mkdir(i)\n",
    "        \n",
    "make_directories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1836ea5b-e330-4de0-bb0c-a7dd64d6a097",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf07b37-777b-4ae9-b591-d72056be7f19",
   "metadata": {},
   "source": [
    "#### retrieve city roster file and text explanation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92f2f34b-bca8-4a2d-8f32-e8e934ae9c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read/format project explanation docx\n",
    "def read_explainer(address = 'A_Input/explanation.docx', txt_len = 50):\n",
    "    \n",
    "    ## read in file\n",
    "    doc = Document(address).paragraphs\n",
    "    doc = [i.text for i in doc]\n",
    "    doc = ' '.join(doc)\n",
    "    \n",
    "    ## wrap text\n",
    "    tw = TextWrapper(width = txt_len, \n",
    "                     replace_whitespace = False, fix_sentence_endings = True,\n",
    "                     break_long_words = False, drop_whitespace = False)\n",
    "    doc = tw.fill(doc)\n",
    "    doc = doc.format('\\n\\n')\n",
    "    return doc\n",
    "    \n",
    "## execute functions\n",
    "city_roster = pd.read_excel('A_Input/city_list.xlsx',\n",
    "    usecols = {'City': str, 'Route': str, 'lon': float, 'lat': float})\n",
    "project_explainer = read_explainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108940fa-42b4-46cf-b7e6-8b317617cf52",
   "metadata": {},
   "source": [
    "#### retrieve links page for first year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "490866ba-715d-4ded-887d-756871df8d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_roster(the_year, the_url = set_url, valid_prefix = set_prefix):\n",
    "  \n",
    "    ## retrive raw web page\n",
    "    the_url = the_url.format(the_year)\n",
    "    with url.urlopen(the_url) as conn:\n",
    "        all_files = conn.read()\n",
    "        conn.close()\n",
    "        \n",
    "    ## extract links to csv files\n",
    "    all_files = BeautifulSoup(all_files)\n",
    "    all_files = all_files.find_all('a')\n",
    "    all_files = [i.string for i in all_files if i.string.find('.csv') != -1]\n",
    "        \n",
    "    ## package as pandas files; limit to valid prefix\n",
    "    all_files = pd.DataFrame({'prefix':0, 'file':all_files,\n",
    "        'lon': np.nan, 'lat': np.nan, 'dist': np.nan, 'city': 'Unknown'})\n",
    "    all_files['prefix'] = [i[0:2] for i in all_files['file']]\n",
    "    all_files = all_files[all_files.prefix.isin(valid_prefix)]\n",
    "    all_files = all_files.reset_index()\n",
    "\n",
    "    del all_files['index']\n",
    "    \n",
    "    return all_files \n",
    "\n",
    "## execute code\n",
    "data_roster = make_data_roster(set_year[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089d7862-e61e-48a3-812c-5aef172d8dbf",
   "metadata": {},
   "source": [
    "#### determine which stations are near route cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d0b283f-24fe-49ef-b186-ba4fb739fe09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache or make: Retrieving cache\n"
     ]
    }
   ],
   "source": [
    "def read_station_data(x, ucl = set_col_list):\n",
    "    the_csv = pd.read_csv(x, usecols = ucl, parse_dates = ['DATE'], dtype = str)\n",
    "    for j in ucl.keys():\n",
    "        if ucl[j] == float:\n",
    "            the_csv[j] = pd.to_numeric(the_csv[j], errors = 'coerce')\n",
    "    the_csv.LATITUDE = the_csv.LATITUDE.fillna(0)\n",
    "    the_csv.LONGITUDE = the_csv.LONGITUDE.fillna(0)\n",
    "    return the_csv\n",
    "\n",
    "def filter_to_proximate_data(\n",
    "    dat_rost = data_roster, the_url = set_url, city = city_roster):\n",
    "\n",
    "    print('Evaluating:', end = ' ')\n",
    "    for i in dat_rost.index:\n",
    "        ## print(dat_rost.loc[i, 'file'], end=', ')\n",
    "        \n",
    "        ## download file and coerce numeric columns to numeric\n",
    "        full_url = set_url.format(set_year[-1]) + dat_rost.loc[i, 'file']\n",
    "        the_csv = read_station_data(full_url)\n",
    "\n",
    "        ## find distance to closest rostered metro area\n",
    "        min_dist = 999999\n",
    "        prox_city = None\n",
    "        for j in city.index:\n",
    "            dist_j = distance(\n",
    "                tuple(the_csv.loc[0, ['LATITUDE', 'LONGITUDE']]),\n",
    "                tuple(city.loc[j, ['lat', 'lon']])).miles\n",
    "            if dist_j < min_dist:\n",
    "                min_dist = dist_j\n",
    "                prox_city = city.loc[j, 'City']\n",
    "        dat_rost.loc[i, 'dist'] = np.round(min_dist)\n",
    "        dat_rost.loc[i, 'city'] = prox_city\n",
    "        \n",
    "    ## save roster file to disk\n",
    "    dat_rost.to_csv('B_Process/station_roster.csv')\n",
    "    return dat_rost\n",
    "\n",
    "def cache_or_make_csv(address, cache_bool, function):\n",
    "    if not cache_bool or not isfile(address):\n",
    "        print('Cache or make: Making file')\n",
    "        x = function()\n",
    "    else:\n",
    "        print('Cache or make: Retrieving cache')\n",
    "        x = pd.read_csv(address)\n",
    "    return x\n",
    "\n",
    "## execution functions\n",
    "station_roster = cache_or_make_csv(\n",
    "    address = 'B_Process/station_roster.csv',\n",
    "    cache_bool = set_cache['station_roster'],\n",
    "    function = filter_to_proximate_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2922e14-8c66-455f-8ba3-ae2aa89c15e4",
   "metadata": {},
   "source": [
    "#### retrieve data from subsequent years for proximate stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2629af6-0762-432a-8b66-4c2f57e4722b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO: Evaluate whether to build an override that forces new download\n"
     ]
    }
   ],
   "source": [
    "def retrieve_station_data(\n",
    "    roster = station_roster, url = set_url, year = set_year, ucl = set_col_list):\n",
    "    \n",
    "    ## download all relavent data files\n",
    "    for i in year:\n",
    "        if not str(i) in roster.columns: roster[str(i)] = False\n",
    "        needed_file = (roster.dist < 30) & ~roster[str(i)]\n",
    "        for j in roster.index[needed_file]:\n",
    "            ## download data file\n",
    "            try:\n",
    "                error_flag = False\n",
    "                x = read_station_data(url.format(i) + roster.loc[j, 'file'])\n",
    "                if i == year[0]:\n",
    "                    roster.loc[j, 'lon'] = x.loc[0, 'LONGITUDE']\n",
    "                    roster.loc[j, 'lat'] = x.loc[0, 'LATITUDE']\n",
    "                x.to_csv(\n",
    "                    'B_Process/downloads/' + str(i) + '_' +\n",
    "                    roster.loc[j, 'file'] + '.gz')\n",
    "            except:\n",
    "                error_flag = True\n",
    "            \n",
    "            ## update roster\n",
    "            if error_flag: roster.loc[j, str(i)] = False\n",
    "            else: roster.loc[j, str(i)] = True\n",
    "            roster.to_csv('B_Process/station_roster.csv')\n",
    "\n",
    "## execution functions\n",
    "if not set_cache['collection']: retrieve_station_data()\n",
    "print('TODO: Evaluate whether to build an override that forces new download')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74bd876-4fc2-45d1-a41d-6e23cb75fb63",
   "metadata": {},
   "source": [
    "#### record completion time stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec88b623-7352-45ed-addd-3632dac6f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_check('ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee54b404-eaa6-4743-8181-caced638143c",
   "metadata": {},
   "source": [
    "# Refine Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba7125-6a8f-45f8-979f-a4cc05a75443",
   "metadata": {},
   "source": [
    "#### Refine and combine files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f953b7ff-3b44-4ad2-83ea-94cb7fa6e6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache or make: Making file\n"
     ]
    }
   ],
   "source": [
    "def compile_weather_data(address = 'B_Process/downloads', ucl = set_col_list):\n",
    "    \n",
    "    ## make list of all available data files\n",
    "    file_list = listdir(address)\n",
    "    file_list = filter(lambda x: x[-7::] == '.csv.gz', file_list)\n",
    "    \n",
    "    ## read in the data\n",
    "    weather_data = []\n",
    "    for i in file_list:\n",
    "        \n",
    "        ## read in file\n",
    "        x = read_station_data('B_Process/downloads/' + i)\n",
    "        x['source'] = i\n",
    "        \n",
    "        ## unpack date information\n",
    "        x['hour'] = x.DATE.dt.hour\n",
    "        x['day'] = x.DATE.dt.dayofyear\n",
    "        x = x.drop('DATE', axis = 1)\n",
    "        x = x.loc[x.day != 366, ]\n",
    "        \n",
    "        ## score weather\n",
    "        for i in ['HourlyPrecipitation', 'HourlyDryBulbTemperature']:\n",
    "            x[i] = x[i].fillna(0)\n",
    "        x['score'] = (x['HourlyDryBulbTemperature'] <= 75) &\\\n",
    "            (x['HourlyDryBulbTemperature'] >= 55) &\\\n",
    "            (x['HourlyPrecipitation'] <= 0.2)\n",
    "        x['score'] = x['score'].astype(int)\n",
    "        x = x.drop(['HourlyDryBulbTemperature', 'HourlyPrecipitation'], axis = 1)\n",
    "        \n",
    "        ## add refined file to data list\n",
    "        weather_data.append(x)\n",
    "        \n",
    "    ## compile as dataframe and save to disk\n",
    "    weather_data = pd.concat(weather_data, axis = 0)\n",
    "    weather_data.to_csv('B_Process/weather_data.csv.gz')\n",
    "    \n",
    "    return weather_data\n",
    "\n",
    "## execute function\n",
    "weather_data = cache_or_make_csv(\n",
    "    address = 'B_Process/weather_data.csv.gz',\n",
    "    cache_bool = set_cache['compilation'],\n",
    "    function = compile_weather_data\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b858194d-9ad8-4823-b5cf-05b8ebb7c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_check('RD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0366c932-ce0d-44da-aa7f-8cda8c66c8fb",
   "metadata": {},
   "source": [
    "# Model Routes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79616312-23ba-4db0-809f-45d9974779d3",
   "metadata": {},
   "source": [
    "#### summarize weather score for each city and then for each route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fda8671e-003c-4bcd-9826-efe798ab4bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## score average temperateness of weather for each city\n",
    "def summarize_by_city(wd = weather_data, sr = station_roster):\n",
    "    \n",
    "    ## assign stations to closest metropolitan area\n",
    "    wd['index'] = [i[5:-3] for i in wd.source]\n",
    "    wd = wd.set_index('index')\n",
    "    sr = sr.set_index('file')\n",
    "    wd = wd.join(sr[['city']])\n",
    "    \n",
    "    ## calculate average weather scores\n",
    "    wd = wd[['city', 'day', 'hour', 'score']]\n",
    "    wd = wd.groupby(['city', 'day', 'hour']).mean().reset_index()\n",
    "    wd['score'] = wd['score'].round(6)\n",
    "    \n",
    "    wd.to_csv('B_Process/city_score.csv')\n",
    "    return wd\n",
    "\n",
    "## calculate average temperateness score across cities in each route\n",
    "def summarize_by_route(cs, cr = city_roster):\n",
    "    \n",
    "    ## merge route assignments\n",
    "    cs = cs.set_index('city')\n",
    "    cr = cr.set_index('City')\n",
    "    cs = cs.join(cr[['Route']])\n",
    "    \n",
    "    ## average scores by route\n",
    "    cs = cs.groupby(['Route', 'day', 'hour']).mean().reset_index()\n",
    "    cs['score'] = cs['score'].round(6)\n",
    "    cs.to_csv('B_Process/route_score.csv')\n",
    "    return cs\n",
    "\n",
    "## add dc to route scores\n",
    "def add_home_city(\n",
    "    rs = route_score, cs = city_score, hc = set_param['home_city']):\n",
    "    hc = cs.loc[cs.city == hc, ]\n",
    "    hc.columns = rs.columns\n",
    "    rs = pd.concat([rs, hc], axis = 0)\n",
    "    return rs\n",
    "\n",
    "## execute code\n",
    "city_score  = summarize_by_city()\n",
    "route_score = summarize_by_route(cs = city_score)\n",
    "route_score = add_home_city()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669ec06b-626b-43fc-a14f-e126809dc11a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### calculate daily summary scores (temperate hours and best time per day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aebad49e-1668-43e0-a0a7-33f508c5cb3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month            1  2  3  4   5   6   7   8   9  10  11  12  weather_class\n",
      "Route                                                                     \n",
      "Alaska State     0  0  0  0   3   7  10  10   2   0   0   0            1.8\n",
      "Milwaukee WI     0  0  1  2   7   9   7   9  10   6   1   0            2.6\n",
      "Minnesota Plus   0  0  2  4   8   8   5   7   9   5   1   0            2.8\n",
      "Oregon Plus      0  1  3  5   9  10   7   7  10   8   2   0            2.9\n",
      "Northeast Plus   0  0  2  5   9   8   5   6   9   8   3   1            3.0\n",
      "Indiana Plus     0  0  3  5   7   6   5   6   9   8   2   1            3.1\n",
      "Wyoming Plus     0  1  4  5   8   6   4   4   8   6   2   0            3.2\n",
      "Washington DC    0  1  4  8   9   4   2   4   9   9   4   2            3.5\n",
      "Missouri Plus    1  1  5  8   7   3   3   4   7   8   4   1            3.7\n",
      "California Plus  6  8  9  9  10   8   6   6   7   9   9   6            4.0\n",
      "Georgia Plus     3  4  7  9   7   2   2   2   5   9   7   6            4.2\n",
      "New Mexico Plus  3  5  7  7   5   2   2   3   5   7   6   3            4.2\n",
      "Louisiana Plus   5  7  9  8   5   1   1   1   3   6   8   6            4.7\n",
      "Hawaii State     8  7  8  5   4   2   1   1   1   1   4   6            5.0\n",
      "Florida State    8  9  7  5   2   1   1   0   1   3   7   8            5.2\n",
      "Puerto Rico      1  1  1  0   0   0   0   0   0   0   0   0            6.0\n"
     ]
    }
   ],
   "source": [
    "## sum hours of good weather per day\n",
    "def sum_temperate_by_day(rs = route_score):\n",
    "    rs = rs.loc[(rs.hour > 6) & (rs.hour < 20), ]\n",
    "    hour_sum = rs.drop('hour', axis = 1).groupby(['Route', 'day']).sum()\n",
    "    hour_sum = hour_sum.round().astype(int).reset_index()\n",
    "    return hour_sum\n",
    "\n",
    "## calculate best travel months\n",
    "def median_temperate_by_month(ds):\n",
    "    \n",
    "    ## convert day of the year into month\n",
    "    ds['month'] = datetime.fromisoformat('2000-01-01')\n",
    "    for i in ds.index:\n",
    "        ds.loc[i, 'month'] = ds.loc[i, 'month'] +\\\n",
    "            timedelta(days = int(ds.loc[i, 'day']))\n",
    "        ds.loc[i, 'month'] = ds.loc[i, 'month'].month\n",
    "        \n",
    "    ## sum hours by month and reshape\n",
    "    ds = ds.groupby(['Route','month']).median().drop('day', axis = 1)\n",
    "    ds = ds.reset_index().pivot(\n",
    "        index = 'Route', columns = 'month', values = 'score').astype(int)\n",
    "    \n",
    "    ## categorize routes on a summer to winter scale\n",
    "    route_weather = np.round(np.array(ds) / 12, 1)\n",
    "    winter_score = np.ones(route_weather.shape) * np.abs(np.arange(1, 13) - 7)\n",
    "    winter_score = winter_score + 1\n",
    "    route_class = (route_weather * winter_score).sum(axis = 1)\n",
    "    route_class = route_class / route_weather.sum(axis = 1)\n",
    "    ds['weather_class'] = np.round(route_class, 1)\n",
    "    ds = ds.sort_values('weather_class')\n",
    "    \n",
    "    return ds\n",
    "\n",
    "## execute code\n",
    "day_score = sum_temperate_by_day()\n",
    "month_score = median_temperate_by_month(day_score.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2887dbc7-fa59-4dc0-9f2e-4a58e5119c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_check('MR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b220a9-e4c8-4498-8219-7855ba983845",
   "metadata": {},
   "source": [
    "# Render Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508d734d-741a-4a0d-9714-20a850903f23",
   "metadata": {},
   "source": [
    "#### Set up plot infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "80be7749-aa60-44a0-bff3-8d46a59134c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApYAAAG+CAYAAAAp5UTMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAI6ElEQVR4nO3ZMW7jQBBFwfbCCbO5Ja/B2/IY2kiAoEDJPlrwqiqdpJMPPJBfM3MbAAD4R3/efQAAAP8HYQkAQEJYAgCQEJYAACSEJQAACWEJAEDi+9Xjtm2z1vqhU+BznOc5MzYGV7jva8bG4AqPG3v2MizXWrPve34QfLrjOGbGxuAK933N2Bhc4XFjz/wKBwAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgMTXzNzefQQAAL+fL5YAACSEJQAACWEJAEBCWAIAkBCWAAAkhCUAAAlhCQBAQlgCAJAQlgAAJIQlAAAJYQkAQEJYAgCQEJYAACSEJQAACWEJAEBCWAIAkBCWAAAkhCUAAAlhCQBA4vvV47Zts9b6oVPgc5znOTM2Ble472vGxuAKjxt79jIs11qz73t+EHy64zhmxsbgCvd9zdgYXOFxY8/8CgcAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAICEsAQAICEsAQBICEsAABLCEgCAhLAEACAhLAEASAhLAAASwhIAgISwBAAgISwBAEgISwAAEsISAIDE18zc3n0EAAC/ny+WAAAkhCUAAAlhCQBAQlgCAJAQlgAAJIQlAACJv0Q6Kd0dftSAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 648x432 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_layout():\n",
    "    \n",
    "    ## make figure\n",
    "    dash_figure = plt.figure(figsize = (9, 6))\n",
    "    dash_grid = dash_figure.add_gridspec(2, 3, figure = dash_figure,\n",
    "        hspace = 0.1, wspace = 0.01, left = 0, right = 1, bottom = 0, top = 1)\n",
    "    dash_figure.set_facecolor('black')\n",
    "    \n",
    "    ## make axes\n",
    "    dash_axis = dict()\n",
    "    dash_axis['hot']   = dash_figure.add_subplot(dash_grid[0, 0])\n",
    "    dash_axis['warm']  = dash_figure.add_subplot(dash_grid[0, 1])\n",
    "    dash_axis['cold']  = dash_figure.add_subplot(dash_grid[0, 2])\n",
    "    dash_axis['home']  = dash_figure.add_subplot(dash_grid[1, 0])\n",
    "    dash_axis['table'] = dash_figure.add_subplot(dash_grid[1, 1])\n",
    "    dash_axis['map']   = dash_figure.add_subplot(dash_grid[1, 2])\n",
    "    \n",
    "    ## remove axis ticks\n",
    "    for i in dash_axis.keys():\n",
    "        dash_axis[i].tick_params(\n",
    "            labelbottom = False, labeltop = False,\n",
    "            labelleft = False, labelright = False,\n",
    "            bottom = False, top = False,\n",
    "            left = False, right = False)\n",
    "        dash_axis[i].set_facecolor('gray')\n",
    "    \n",
    "##\n",
    "make_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5668f6-9b20-4565-a066-379072231ca1",
   "metadata": {},
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a1f5c76-8a6a-4edb-afa2-6e6302713e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_check('RV')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23023019-0f8e-4cc7-b266-ccb6fe20f7b1",
   "metadata": {},
   "source": [
    "# Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c9c6ff3-02a3-4b1d-91e9-acfc85b54b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_check('TC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de2aab-a8a1-4dff-bd0a-bd051ca73d52",
   "metadata": {},
   "source": [
    "# Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a350ac31-ae36-4b98-9865-a387f256cf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Check AA   = 13:28:58\n",
      "Time Check ID   = 13:29:04\n",
      "Time Check RD   = 13:37:03\n",
      "Time Check MR   = 13:37:38\n",
      "Time Check RV   = 13:37:38\n",
      "Time Check TC   = 13:37:38\n",
      "Time Check ZZ   = 13:37:38\n"
     ]
    }
   ],
   "source": [
    "## Display time statistics\n",
    "time_check('ZZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b480b729-8af3-42da-977a-4836801a32e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
